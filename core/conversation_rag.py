# Generated by: Ollama Minimax (llm_router)
# Fixed by: Claude Code (embedding storage)
"""
Conversation RAG - Index and query Claude Code session logs

Indexes .jsonl session files into searchable vector database.
Uses local embeddings (all-MiniLM-L6-v2) for semantic search.

GOVERNANCE: 100% local. No data leaves machine.
CHECKSUM: ΔΣ=42
"""

import sqlite3
import json
import os
import hashlib
from typing import List, Dict, Any

import embeddings

DB_PATH = "data/conversation_memory.db"


def init_db():
    """Create tables if not exist."""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS conversation_chunks (
            chunk_id TEXT PRIMARY KEY,
            session_id TEXT NOT NULL,
            timestamp TEXT,
            role TEXT,
            content TEXT NOT NULL,
            embedding BLOB,
            metadata TEXT
        )
    """)
    
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_session_id ON conversation_chunks(session_id)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON conversation_chunks(timestamp)")
    
    conn.commit()
    conn.close()


def _extract_text(content) -> str:
    """Extract text from various content formats."""
    if isinstance(content, str):
        return content
    
    if isinstance(content, list):
        text_parts = []
        for part in content:
            if isinstance(part, dict):
                if part.get("type") == "text":
                    text_parts.append(part.get("text", ""))
                elif part.get("type") == "tool_result":
                    tool_content = part.get("content", "")
                    if isinstance(tool_content, list):
                        tool_content = _extract_text(tool_content)
                    text_parts.append(f"[Tool Result]: {tool_content}")
        return " ".join(text_parts)
    
    return str(content)


def _chunk_conversation(messages: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """
    Split long conversations into semantic chunks (500-1000 chars).
    Chunk based on conversation turns.
    """
    chunks = []
    current_chunk = []
    current_length = 0
    
    for msg in messages:
        content = _extract_text(msg.get("content", ""))
        msg_length = len(content)
        
        if current_length + msg_length > 1000 and current_chunk:
            chunks.append(current_chunk)
            current_chunk = [msg]
            current_length = msg_length
        else:
            current_chunk.append(msg)
            current_length += msg_length
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


def index_session(jsonl_path: str) -> Dict[str, Any]:
    """
    Parse .jsonl file, chunk conversations, embed, store.
    
    Returns:
        {"success": bool, "chunks_indexed": int, "session_id": str}
    """
    if not os.path.exists(jsonl_path):
        return {"success": False, "error": f"File not found: {jsonl_path}"}
    
    session_id = os.path.splitext(os.path.basename(jsonl_path))[0]
    
    # Parse jsonl
    messages = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    messages.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    
    if not messages:
        return {"success": False, "error": "No valid messages in file"}
    
    # Chunk
    chunks = _chunk_conversation(messages)
    
    # Store
    init_db()
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    chunks_indexed = 0
    
    for chunk in chunks:
        content_parts = []
        timestamp = None
        roles = []
        
        for msg in chunk:
            msg_type = msg.get("type", msg.get("role", "unknown"))
            roles.append(msg_type)
            content = _extract_text(msg.get("content", ""))
            content_parts.append(f"{msg_type}: {content}")
            
            if not timestamp and msg.get("timestamp"):
                timestamp = msg["timestamp"]
        
        content_text = "\n".join(content_parts)
        chunk_id = hashlib.md5(f"{session_id}:{content_text[:100]}".encode()).hexdigest()
        
        # Embed using existing embeddings.py
        embedding_bytes = embeddings.embed(content_text)
        if not embedding_bytes:
            continue  # Skip if embedding failed
        
        role = chunk[0].get("type", chunk[0].get("role", "unknown"))
        metadata = json.dumps({
            "message_count": len(chunk),
            "roles": roles
        })
        
        cursor.execute("""
            INSERT OR REPLACE INTO conversation_chunks 
            (chunk_id, session_id, timestamp, role, content, embedding, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (chunk_id, session_id, timestamp, role, content_text, embedding_bytes, metadata))
        chunks_indexed += 1
    
    conn.commit()
    conn.close()
    
    return {"success": True, "chunks_indexed": chunks_indexed, "session_id": session_id}


def query(question: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    Embed question, find similar chunks, return results.
    
    Args:
        question: Query text
        top_k: Number of results to return
    
    Returns:
        List of dicts with: chunk_id, session_id, content, similarity, metadata
    """
    question_embedding = embeddings.embed(question)
    if not question_embedding:
        return []  # Embedding failed
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT chunk_id, session_id, timestamp, role, content, embedding, metadata 
        FROM conversation_chunks 
        WHERE embedding IS NOT NULL
    """)
    rows = cursor.fetchall()
    conn.close()
    
    results = []
    for row in rows:
        chunk_id, session_id, timestamp, role, content, embedding_bytes, metadata = row
        
        similarity = embeddings.cosine_similarity(question_embedding, embedding_bytes)
        
        results.append({
            "chunk_id": chunk_id,
            "session_id": session_id,
            "timestamp": timestamp,
            "role": role,
            "content": content[:500] + "..." if len(content) > 500 else content,
            "similarity": similarity,
            "metadata": json.loads(metadata) if metadata else {}
        })
    
    results.sort(key=lambda x: x["similarity"], reverse=True)
    return results[:top_k]


def get_stats() -> Dict[str, Any]:
    """Get RAG database statistics."""
    if not os.path.exists(DB_PATH):
        return {"indexed": False}
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("SELECT COUNT(DISTINCT session_id) FROM conversation_chunks")
    session_count = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM conversation_chunks")
    chunk_count = cursor.fetchone()[0]
    
    conn.close()
    
    return {
        "indexed": True,
        "sessions": session_count,
        "chunks": chunk_count,
        "db_path": DB_PATH
    }
