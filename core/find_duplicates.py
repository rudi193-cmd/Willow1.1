# Generated by: Ollama Minimax (llm_router)
import os
import hashlib
from collections import defaultdict

def should_skip_path(path):
    """Check if path should be skipped (Windows special paths, etc.)"""
    if "\\\\.\\\\" in path or path.endswith("nul"):
        return True
    return False

def get_relative_path(file_path, base_path):
    """Get relative path with error handling for Windows compatibility"""
    try:
        return os.path.relpath(file_path, base_path)
    except (ValueError, OSError):
        return None

def hash_file(file_path, chunk_size=65536):
    """Hash a binary file using SHA256"""
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(chunk_size), b""):
                sha256_hash.update(chunk)
        return sha256_hash.hexdigest()
    except (OSError, IOError):
        return None

def is_image_file(filename):
    """Check if file is an image based on extension"""
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.svg', '.ico', '.psd', '.ai'}
    return os.path.splitext(filename)[1].lower() in image_extensions

def scan_directory(base_path):
    """Scan directory and return file hashes grouped by hash"""
    file_hashes = defaultdict(list)
    
    skip_dirs = {'__pycache__', '.git', 'venv', 'env', '.venv', '.env', 'node_modules', '.idea', '.vscode'}
    
    for root, dirs, files in os.walk(base_path):
        dirs[:] = [d for d in dirs if d not in skip_dirs]
        
        for filename in files:
            file_path = os.path.join(root, filename)
            
            if should_skip_path(file_path):
                continue
            
            rel_path = get_relative_path(file_path, base_path)
            if rel_path is None:
                continue
            
            file_hash = hash_file(file_path)
            if file_hash:
                file_hashes[file_hash].append((rel_path, file_path, filename))
    
    return file_hashes

def find_duplicates(file_hashes):
    """Find files with duplicate hashes"""
    duplicates = {}
    for file_hash, files in file_hashes.items():
        if len(files) > 1:
            duplicates[file_hash] = files
    return duplicates

def report_duplicates(duplicates, title, image_only=False):
    """Report duplicate files"""
    print(f"\n{title}")
    print("=" * 60)
    
    found = False
    for file_hash, files in sorted(duplicates.items()):
        is_image = any(is_image_file(f[2]) for f in files)
        
        if image_only and not is_image:
            continue
        
        found = True
        print(f"\nHash: {file_hash}")
        for rel_path, full_path, filename in files:
            print(f"  {rel_path}")
            if is_image_file(filename):
                print(f"    [IMAGE]")
    
    if not found:
        print("None found.")

def report_untitled(file_hashes):
    """Report all Untitled files"""
    print("\n=== UNTITLED FILES ===")
    print("=" * 60)
    
    found = False
    for file_hash, files in sorted(file_hashes.items()):
        for rel_path, full_path, filename in files:
            if filename.startswith("Untitled."):
                found = True
                print(f"  {rel_path}")
                if is_image_file(filename):
                    print(f"    [IMAGE]")
    
    if not found:
        print("None found.")

def main():
    """Main function to scan repositories for duplicate files"""
    repos = [
        r"C:\Users\Sean\Documents\GitHub\Willow",
        r"C:\Users\Sean\Documents\GitHub\die-namic-system"
    ]
    
    all_hashes = defaultdict(list)
    
    for repo in repos:
        if not os.path.exists(repo):
            print(f"Warning: Repository not found: {repo}")
            continue
        
        print(f"Scanning: {repo}")
        repo_hashes = scan_directory(repo)
        
        for file_hash, files in repo_hashes.items():
            all_hashes[file_hash].extend(files)
    
    duplicates = find_duplicates(all_hashes)
    
    print(f"\nTotal unique file hashes: {len(all_hashes)}")
    print(f"Total duplicate groups: {len(duplicates)}")
    
    report_duplicates(duplicates, "=== ALL DUPLICATES ===", image_only=False)
    
    report_duplicates(duplicates, "=== IMAGE DUPLICATES ===", image_only=True)
    
    report_untitled(all_hashes)

if __name__ == "__main__":
    main()