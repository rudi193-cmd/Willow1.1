"""
Agent Engine - Conversational AI with Tool Access

Universal conversational agent system for all Willow agents.
Replaces task-executor model with natural conversation + tools.

GOVERNANCE: All tool calls gated through tool_engine + gate.py
COST: Free-tier-first routing, $0.10/month/user cap
AUTHOR: Willow Agent System
VERSION: 2.0
CHECKSUM: ΔΣ=42
"""

import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any, Generator

# Core imports
from core import llm_router, tool_engine, agent_registry
from core.n2n_packets import N2NPacket, PacketType, create_handoff, create_delta
from core.n2n_db import N2NDatabase
from core.time_resume_capsule import TimeResumeCapsule
from core.recursion_tracker import RecursionTracker
from core.workflow_state import WorkflowDetector


class AgentEngine:
    """
    Conversational AI agent with tool access and governance.

    Works for any agent: Willow, Kart, Jane, Riggs, etc.
    Each agent has its own personality, tools, and constraints.
    """

    def __init__(self, username: str, agent_name: str = "willow"):
        """
        Initialize agent engine.

        Args:
            username: User name
            agent_name: Agent name (willow, kart, jane, etc.)
        """
        self.username = username
        self.agent_name = agent_name

        # Load agent info
        self.agent_info = agent_registry.get_agent(username, agent_name)
        if not self.agent_info:
            raise ValueError(f"Agent '{agent_name}' not registered for user '{username}'")

        self.trust_level = self.agent_info.get("trust_level", "WORKER")
        self.agent_type = self.agent_info.get("agent_type", "persona")

        # Load available tools
        self.tools = tool_engine.list_tools(agent_name, username)

        # Load agent personality from AGENT_PROFILE.md
        self.system_prompt = self._load_agent_profile()

        # Conversation history
        self.context = []

        # Cost tracking
        self.api_tier = "free"  # Always free tier for $0.10/month goal
        
        # N2N communication
        self.n2n_db = N2NDatabase(username)
        self.node_id = f"{agent_name}@{username}"
        
        # Session tracking
        self.time_capsule = TimeResumeCapsule(username)
        self.recursion_tracker = RecursionTracker()
        self.workflow_detector = WorkflowDetector(auto_detect_enabled=True)


    def send_n2n_packet(self, target_agent: str, packet_type: PacketType, payload: dict) -> str:
        """
        Send N2N packet to another agent.
        
        Args:
            target_agent: Target agent name
            packet_type: Type of packet (PacketType enum)
            payload: Packet payload (minimal data)
            
        Returns:
            packet_id
        """
        target_node = f"{target_agent}@{self.username}"
        
        packet = N2NPacket.create_packet(
            packet_type=packet_type,
            source_node=self.node_id,
            target_node=target_node,
            payload=payload,
            authority="ai",
            scope="local"
        )
        
        packet_id = self.n2n_db.send_packet(packet)
        return packet_id
    
    def receive_n2n_packets(self, status: str = "SENT") -> list:
        """
        Receive N2N packets addressed to this agent.
        
        Args:
            status: Packet status filter (SENT, RECEIVED, ACKNOWLEDGED)
            
        Returns:
            List of packets
        """
        packets = self.n2n_db.receive_packets(self.node_id, status=status)
        
        # Mark as received
        for packet in packets:
            self.n2n_db.mark_received(packet["packet_id"])
        
        return packets
    
    def send_handoff(self, target_agent: str, what_happened: str, what_next: str) -> str:
        """Send HANDOFF packet (minimal context transfer)."""
        payload = {"what_happened": what_happened, "what_next": what_next}
        return self.send_n2n_packet(target_agent, PacketType.HANDOFF, payload)

    def chat(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Any:
        """
        Conversational chat with tool access.

        Args:
            user_message: User's message
            conversation_history: Optional previous conversation context
            stream: If True, return generator for streaming response

        Returns:
            If stream=False: {"response": str, "tool_calls": list, "provider": str, "tier": str}
            If stream=True: Generator yielding response chunks
        """
        # Build context
        if conversation_history:
            self.context = conversation_history

        # Add system prompt if not present
        if not self.context or self.context[0].get("role") != "system":
            self.context.insert(0, {
                "role": "system",
                "content": self.system_prompt
            })

        # Add user message
        self.context.append({
            "role": "user",
            "content": user_message
        })

        # Get response from LLM with tool awareness
        if stream:
            return self._chat_streaming()
        else:
            return self._chat_blocking()

    def _chat_blocking(self) -> Dict[str, Any]:
        """Non-streaming chat response."""
        # Build prompt from context
        prompt = self._build_prompt()

        # Call LLM (free tier only)
        try:
            response = llm_router.ask(
                prompt,
                preferred_tier="free"
            )

            if not response:
                return {
                    "response": "I'm having trouble connecting to my language models. Please try again.",
                    "tool_calls": [],
                    "error": "LLM request failed"
                }

            # Parse response for tool calls
            content = response.content.strip()
            tool_calls = self._extract_tool_calls(content)

            # Execute any tool calls
            tool_results = []
            if tool_calls:
                for tool_call in tool_calls:
                    result = self._execute_tool(tool_call)
                    tool_results.append(result)

                    # If tool requires approval, return early
                    if result.get("governance_status") == "PENDING_APPROVAL":
                        return {
                            "response": f"I need approval to use the '{tool_call['tool']}' tool. Please check the governance dashboard.",
                            "tool_calls": tool_results,
                            "provider": response.provider,
                            "tier": response.tier,
                            "pending_approval": True,
                            "request_id": result.get("request_id")
                        }

                # Add tool results to context and get final response
                self.context.append({
                    "role": "assistant",
                    "content": content
                })

                tool_summary = "\n\n".join([
                    f"Tool: {r['tool']}\nResult: {json.dumps(r.get('result'), indent=2)}"
                    for r in tool_results
                ])

                self.context.append({
                    "role": "user",
                    "content": f"[Tool Results]\n{tool_summary}\n\nPlease respond to the user based on these results."
                })

                # Get final response
                final_prompt = self._build_prompt()
                final_response = llm_router.ask(
                    final_prompt,
                    preferred_tier="free"
                )

                return {
                    "response": final_response.content.strip(),
                    "tool_calls": tool_results,
                    "provider": final_response.provider,
                    "tier": final_response.tier
                }

            # No tool calls, return direct response
            self.context.append({
                "role": "assistant",
                "content": content
            })

            return {
                "response": content,
                "tool_calls": [],
                "provider": response.provider,
                "tier": response.tier
            }

        except Exception as e:
            return {
                "response": f"I encountered an error: {str(e)}",
                "tool_calls": [],
                "error": str(e)
            }

    def _chat_streaming(self) -> Generator[str, None, None]:
        """Streaming chat response (for SSE)."""
        # TODO: Implement streaming support
        # For now, fall back to blocking
        result = self._chat_blocking()
        yield json.dumps(result)

    def _build_prompt(self) -> str:
        """Build prompt from conversation context."""
        return "\n\n".join([
            f"{msg['role'].upper()}: {msg['content']}"
            for msg in self.context
        ])

    def _load_agent_profile(self) -> str:
        """Load agent personality from AGENT_PROFILE.md."""
        profile_path = Path(self.agent_info.get("profile_path", ""))

        if profile_path.exists():
            profile_content = profile_path.read_text()
        else:
            # Default profile if not found
            profile_content = f"# Agent Profile: {self.agent_name}\nNo detailed profile available."

        # Build conversational system prompt
        tools_list = "\n".join([
            f"- **{t['name']}**: {t['description']}. Params: {t['parameters']}"
            for t in self.tools
        ])

        return f"""{profile_content}

## Conversational Instructions

You are {self.agent_info.get('display_name', self.agent_name)}, a conversational AI agent.

**Your role:** {self.agent_info.get('agent_type', 'assistant')}
**Trust level:** {self.trust_level}
**Current user:** {self.username}

### Available Tools

You have access to these tools:
{tools_list if tools_list else "(No tools available)"}

### How to Use Tools

When you need to use a tool, include it in your response like this:

```tool
{{"tool": "tool_name", "params": {{"param1": "value1"}}}}
```

You can have a normal conversation AND use tools in the same response. For example:

"Let me check that file for you.

```tool
{{"tool": "read_file", "params": {{"file_path": "example.txt"}}}}
```

I'll read the file and let you know what I find."

### Conversation Style

- Be natural and conversational (like Claude Code)
- Explain what you're doing and why
- Ask clarifying questions when needed
- Show your reasoning process
- Be helpful but honest about limitations
- If you can't do something, explain why

### Governance

All tool calls are governance-checked. Some operations require human approval.
If approval is needed, I'll let the user know and pause until they approve.

### Cost Awareness

We use free-tier AI providers to keep costs at $0.10/month per user.
Prefer efficient, clear communication over lengthy responses.
"""

    def _extract_tool_calls(self, content: str) -> List[Dict]:
        """Extract tool calls from LLM response."""
        tool_calls = []

        # Look for ```tool blocks
        if "```tool" in content:
            parts = content.split("```tool")
            for part in parts[1:]:  # Skip first part (before any tool block)
                if "```" in part:
                    tool_json = part.split("```")[0].strip()
                    try:
                        tool_call = json.loads(tool_json)
                        if "tool" in tool_call:
                            tool_calls.append(tool_call)
                    except json.JSONDecodeError:
                        pass  # Skip invalid JSON

        return tool_calls

    def _execute_tool(self, tool_call: Dict) -> Dict:
        """Execute a tool call via tool_engine."""
        tool_name = tool_call.get("tool")
        params = tool_call.get("params", {})

        try:
            result = tool_engine.execute(
                tool_name=tool_name,
                params=params,
                agent=self.agent_name,
                username=self.username
            )

            return {
                "tool": tool_name,
                "params": params,
                "result": result,
                "governance_status": result.get("governance_status", "APPROVED"),
                "request_id": result.get("request_id")
            }

        except Exception as e:
            return {
                "tool": tool_name,
                "params": params,
                "result": {"success": False, "error": str(e)},
                "governance_status": "ERROR"
            }

    def reset_context(self):
        """Clear conversation history (start new session)."""
        self.context = []


def chat(
    username: str,
    agent_name: str,
    message: str,
    conversation_history: Optional[List[Dict]] = None,
    stream: bool = False
) -> Any:
    """
    Convenience function for one-off agent chat.

    Args:
        username: User name
        agent_name: Agent name (willow, kart, jane, etc.)
        message: User message
        conversation_history: Optional conversation context
        stream: Enable streaming response

    Returns:
        Chat response dict or generator
    """
    engine = AgentEngine(username, agent_name)
    return engine.chat(message, conversation_history, stream)
