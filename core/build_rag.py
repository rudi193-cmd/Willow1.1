# Generated by: Ollama Minimax (llm_router)

import os
import re
import sqlite3
import ast
from pathlib import Path

REPOS = {
    'willow': 'C:/Users/Sean/Documents/GitHub/Willow/',
    'die-namic': 'C:/Users/Sean/Documents/GitHub/die-namic-system/'
}

SKIP_DIRS = {'__pycache__', '.git', '.pytest_cache', 'venv', 'env', 'vendor', 'node_modules'}
SKIP_PATTERNS = {'test_', '_test.', '.pyc'}
DB_PATH = 'C:/Users/Sean/Documents/GitHub/Willow/core/knowledge.db'


def init_db():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('''
        CREATE TABLE IF NOT EXISTS chunks (
            chunk_id INTEGER PRIMARY KEY AUTOINCREMENT,
            chunk_text TEXT NOT NULL,
            repo TEXT,
            file_path TEXT,
            type TEXT,
            entity_name TEXT,
            line_start INTEGER,
            line_end INTEGER,
            embedding BLOB
        )
    ''')
    conn.commit()
    return conn


def get_line_number(content, start_pos):
    return content[:start_pos].count('\n') + 1


def extract_py_chunks(file_path, content, repo_name):
    chunks = []
    try:
        tree = ast.parse(content)
    except SyntaxError:
        return chunks

    module_doc = ast.get_docstring(tree)
    if module_doc:
        start_line = 1
        end_line = len(content.split('\n'))
        chunks.append({
            'text': module_doc,
            'metadata': {
                'repo': repo_name,
                'file_path': file_path,
                'type': 'doc',
                'entity_name': 'module',
                'line_range': (start_line, end_line)
            }
        })

    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            class_doc = ast.get_docstring(node)
            if class_doc:
                chunks.append({
                    'text': class_doc,
                    'metadata': {
                        'repo': repo_name,
                        'file_path': file_path,
                        'type': 'class',
                        'entity_name': node.name,
                        'line_range': (node.lineno, node.end_lineno or node.lineno)
                    }
                })

        if isinstance(node, ast.FunctionDef):
            func_doc = ast.get_docstring(node)
            if func_doc:
                chunks.append({
                    'text': func_doc,
                    'metadata': {
                        'repo': repo_name,
                        'file_path': file_path,
                        'type': 'function',
                        'entity_name': node.name,
                        'line_range': (node.lineno, node.end_lineno or node.lineno)
                    }
                })

    return chunks


def extract_md_chunks(file_path, content, repo_name):
    chunks = []
    sections = re.split(r'^(##\s+.+)$', content, flags=re.MULTILINE)

    if sections[0].strip():
        chunks.append({
            'text': sections[0].strip(),
            'metadata': {
                'repo': repo_name,
                'file_path': file_path,
                'type': 'doc',
                'entity_name': 'intro',
                'line_range': (1, content.count('\n') + 1)
            }
        })

    for i in range(1, len(sections), 2):
        header = sections[i]
        body = sections[i + 1] if i + 1 < len(sections) else ''
        header_name = header.replace('##', '').strip()
        start_line = content[:content.find(header)].count('\n') + 1
        end_line = start_line + content[content.find(header):].count('\n')

        if body.strip():
            chunks.append({
                'text': f"{header_name}\n\n{body.strip()}",
                'metadata': {
                    'repo': repo_name,
                    'file_path': file_path,
                    'type': 'spec',
                    'entity_name': header_name,
                    'line_range': (start_line, end_line)
                }
            })

    return chunks


def should_skip(path):
    path_str = str(path)
    for skip_dir in SKIP_DIRS:
        if skip_dir in path_str:
            return True
    for pattern in SKIP_PATTERNS:
        if pattern in path_str:
            return True
    return False


def extract_chunks(repo_path):
    repo_name = None
    for name, path in REPOS.items():
        if repo_path.strip('\\/') in path.strip('\\/'):
            repo_name = name
            break

    if not repo_name:
        for name, path in REPOS.items():
            if path.startswith(repo_path[:30]):
                repo_name = name
                break

    if not repo_name:
        return []

    all_chunks = []
    repo_dir = Path(repo_path)

    for root, dirs, files in os.walk(repo_dir):
        dirs[:] = [d for d in dirs if d not in SKIP_DIRS]

        for file in files:
            file_path = os.path.join(root, file)

            if should_skip(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except (UnicodeDecodeError, IOError):
                continue

            try:
            rel_path = os.path.relpath(file_path, repo_dir)
                except (ValueError, OSError):
                    continue

            if file.endswith('.py'):
                chunks = extract_py_chunks(rel_path, content, repo_name)
                all_chunks.extend(chunks)

            elif file.endswith('.md'):
                chunks = extract_md_chunks(rel_path, content, repo_name)
                all_chunks.extend(chunks)

    return all_chunks


def embed_chunk(text, entity_name):
    try:
        from llm_router import ask
        prompt = f"Summarize this code: {text}"
        response = ask(prompt)
        return response
    except ImportError:
        return f"Embedding placeholder for: {entity_name}"


def index_repo(repo_path, repo_name):
    chunks = extract_chunks(repo_path)

    conn = init_db()
    cur = conn.cursor()

    indexed_count = 0

    for chunk in chunks:
        text = chunk['text']
        metadata = chunk['metadata']

        embedding = embed_chunk(text, metadata['entity_name'])

        cur.execute('''
            INSERT INTO chunks (chunk_text, repo, file_path, type, entity_name, line_start, line_end, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            text,
            metadata['repo'],
            metadata['file_path'],
            metadata['type'],
            metadata['entity_name'],
            metadata['line_range'][0],
            metadata['line_range'][1],
            embedding
        ))

        indexed_count += 1

    conn.commit()
    conn.close()

    return indexed_count


def search_rag(query, limit=5):
    conn = init_db()
    cur = conn.cursor()

    try:
        from llm_router import ask
        query_embedding = ask(f"Summarize this query: {query}")
    except ImportError:
        query_embedding = f"Query: {query}"

    cur.execute('''
        SELECT chunk_text, repo, file_path, type, entity_name, line_start, line_end, embedding
        FROM chunks
        ORDER BY RANDOM()
        LIMIT ?
    ''', (limit,))

    results = []

    for row in cur.fetchall():
        chunk_text, repo, file_path, chunk_type, entity_name, line_start, line_end, embedding = row

        relevance = 1.0 if query.lower() in chunk_text.lower() else 0.5

        results.append({
            'text': chunk_text,
            'metadata': {
                'repo': repo,
                'file_path': file_path,
                'type': chunk_type,
                'entity_name': entity_name,
                'line_range': (line_start, line_end)
            },
            'relevance_score': relevance
        })

    conn.close()

    results.sort(key=lambda x: x['relevance_score'], reverse=True)

    return results[:limit]


def main():
    print("Starting RAG indexing...")

    willow_count = index_repo(REPOS['willow'], 'willow')
    print(f"Indexed {willow_count} chunks from Willow repo")

    dinamic_count = index_repo(REPOS['die-namic'], 'die-namic')
    print(f"Indexed {dinamic_count} chunks from Die-namic repo")

    total = willow_count + dinamic_count
    print(f"Total chunks indexed: {total}")

    print("\nSample search test:")
    results = search_rag("function definition", limit=3)
    for i, res in enumerate(results, 1):
        print(f"{i}. {res['metadata']['entity_name']} ({res['metadata']['type']}) - Score: {res['relevance_score']:.2f}")


if __name__ == '__main__':
    main()